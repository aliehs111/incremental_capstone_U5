{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7132cba-9acb-4746-b6a6-06c280aaa5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (50000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Imports & Load Raw Data\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/bike_rental_reviews.csv', encoding='latin-1')\n",
    "print(\"Raw data shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4733ab-afb2-461c-ad21-6ea314bfc2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sheilamcgovern/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sheilamcgovern/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/sheilamcgovern/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied cleaning. Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The entire process was easy, and the availabil...</td>\n",
       "      <td>entire process easy availability high quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Standard rental process. The mobile app was ac...</td>\n",
       "      <td>standard rental process mobile app acceptable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of the best bike rentals Iâve had. The m...</td>\n",
       "      <td>one best bike rental ive mobile app made even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the best bike rentals Iâve had. The c...</td>\n",
       "      <td>one best bike rental ive customer service made...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not worth the money. The seat comfort was a ma...</td>\n",
       "      <td>worth money seat comfort major letdown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  \\\n",
       "0  The entire process was easy, and the availabil...   \n",
       "1  Standard rental process. The mobile app was ac...   \n",
       "2  One of the best bike rentals Iâve had. The m...   \n",
       "3  One of the best bike rentals Iâve had. The c...   \n",
       "4  Not worth the money. The seat comfort was a ma...   \n",
       "\n",
       "                                      cleaned_review  \n",
       "0      entire process easy availability high quality  \n",
       "1      standard rental process mobile app acceptable  \n",
       "2  one best bike rental ive mobile app made even ...  \n",
       "3  one best bike rental ive customer service made...  \n",
       "4             worth money seat comfort major letdown  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text Cleaning Setup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# download once\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    return \" \".join(lemmatizer.lemmatize(t) for t in tokens if t not in stop_words)\n",
    "\n",
    "# apply cleaning\n",
    "df['cleaned_review'] = df['review_text'].astype(str).apply(clean_text)\n",
    "print(\"Applied cleaning. Sample:\")\n",
    "display(df[['review_text','cleaned_review']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab211ea-5511-4296-ad1d-675411499905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deduplication: (300, 3)\n"
     ]
    }
   ],
   "source": [
    "# Drop Duplicates\n",
    "# keep first occurrence of each unique cleaned_review\n",
    "df_unique = df.drop_duplicates(subset='cleaned_review').reset_index(drop=True)\n",
    "print(\"After deduplication:\", df_unique.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a933dc9-1b67-4688-9882-00957e726a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (300, 115)\n",
      "Unique label counts:\n",
      " sentiment\n",
      "positive    100\n",
      "neutral     100\n",
      "negative    100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Vectorize with TF–IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_unique = vectorizer.fit_transform(df_unique['cleaned_review'])\n",
    "y_unique = df_unique['sentiment']\n",
    "\n",
    "print(\"Feature matrix shape:\", X_unique.shape)\n",
    "print(\"Unique label counts:\\n\", y_unique.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a399c15-3b43-404d-88af-23574299de17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved deduped DataFrame and TF–IDF artifacts to ../data/\n"
     ]
    }
   ],
   "source": [
    "# Persist Deduplicated Artifacts\n",
    "import pickle\n",
    "\n",
    "with open('../data/df_unique.pkl',    'wb') as f: pickle.dump(df_unique,      f)\n",
    "with open('../data/X_unique.pkl',     'wb') as f: pickle.dump(X_unique,       f)\n",
    "with open('../data/y_unique.pkl',     'wb') as f: pickle.dump(y_unique,       f)\n",
    "with open('../data/vectorizer_dup.pkl','wb') as f: pickle.dump(vectorizer,    f)\n",
    "\n",
    "print(\"Saved deduped DataFrame and TF–IDF artifacts to ../data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b7ffe-62a2-4322-9e69-c92268059e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
